{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLnGV/vzvoTDfz+JpXjSJz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elangbijak4/Adaptive-Stratified-Sampling---ASS/blob/main/Perbedaan_PCA_vs_TabNet_Filosofi_dan_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SEKILAS PERBANDINGAN DENGAN PCA\n",
        "PCA memilih fitur untuk konteks secara global, jadi tampak linier, sekali pilih maka itu berlaku global, tetapi TabNet menyesuaikan per sampel, untuk setiap sampel mungkin memiliki fitur terpilih yang berbeda, sehingga secara global dia memilih fitur secara dinamis, berubah-ubah sesuai sampel, sehingga secara global dia tidak linier."
      ],
      "metadata": {
        "id": "hIUPN9jYtl59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install dependencies (run once)\n",
        "# If you want to use GPU, set Colab runtime to GPU before running.\n",
        "\n",
        "# CPU-safe PyTorch install (let pip decide best build)\n",
        "!pip install -q torch torchvision torchaudio\n",
        "\n",
        "# Install TabNet from GitHub (recommended for compatibility)\n",
        "!pip install -q git+https://github.com/dreamquark-ai/tabnet.git\n",
        "\n",
        "# Utilities\n",
        "!pip install -q scikit-learn pandas matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WvKBRE-apO5",
        "outputId": "76a23c1d-ab77-42db-d0b6-0bba2d5d92cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concept primer — what TabNet is (short)\n",
        "\n",
        "**High-level idea:** TabNet is a deep neural architecture for tabular data that uses *sequential, sparse attention* to decide which features to use at each decision step.  \n",
        "Key components:\n",
        "\n",
        "- **Feature Transformer (FT):** a small feed-forward network that transforms selected features into a latent representation.\n",
        "- **Attentive Transformer (AT):** computes a *mask* (attention) over input features at each decision step, telling the model which features to focus on at that step.\n",
        "- **Decision Steps (N steps):** at each step the model selects a subset of features via the AT, processes them through FT, and accumulates a partial decision output; final output is aggregated across steps.\n",
        "- **Sparse masks:** enforced via `sparsemax` (or `entmax`), encouraging the model to use few important features per step — this yields interpretability (feature masks per-step).\n",
        "- **Why it helps:** adaptive, step-wise feature selection makes the model both powerful and interpretable; it can learn different “views” of the data across steps.\n"
      ],
      "metadata": {
        "id": "27udm9SbqZIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: imports + synthetic dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import random\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Synthetic dataset: 6 features with known linear weights + non-linear noise\n",
        "n = 6000\n",
        "X = np.random.normal(0, 1, size=(n, 6))\n",
        "true_w = np.array([1.2, -0.8, 0.0, 0.5, 0.0, 0.2])  # only some features matter strongly\n",
        "logit = X.dot(true_w) + 0.3 * np.sin(X[:,0])  # add small non-linearity\n",
        "probs = 1 / (1 + np.exp(-logit))\n",
        "y = (probs > 0.5).astype(int)\n",
        "\n",
        "# Optional: create a \"stratum\" column to visualize later — not needed for TabNet itself\n",
        "strata = (X[:,0] > 0).astype(int)  # simple split\n",
        "df = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])\n",
        "df[\"y\"] = y\n",
        "df[\"stratum\"] = strata\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=0.25, stratify=df[\"y\"], random_state=0)\n",
        "print(\"train shape:\", train_df.shape, \"val shape:\", val_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFTs0l9SqmcY",
        "outputId": "1a7132c9-0ef0-418c-e31b-3c730aab344c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape: (4500, 8) val shape: (1500, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: prepare arrays + set TabNet hyperparameters\n",
        "feature_cols = [c for c in train_df.columns if c.startswith(\"f\")]\n",
        "X_train = train_df[feature_cols].values\n",
        "y_train = train_df[\"y\"].values.astype(int)\n",
        "X_val = val_df[feature_cols].values\n",
        "y_val = val_df[\"y\"].values.astype(int)\n",
        "\n",
        "# TabNet hyperparameters (explain meaning)\n",
        "# n_d, n_a: width of decision and attention embeddings (controls capacity)\n",
        "# n_steps: number of decision steps (how many sequential \"views\" model will create)\n",
        "# mask_type: 'sparsemax' encourages sparse masks -> interpretability\n",
        "tabnet_params = dict(\n",
        "    n_d=8,        # dimension of decision representation\n",
        "    n_a=8,        # dimension of attention representation\n",
        "    n_steps=5,    # number of sequential decision steps\n",
        "    gamma=1.5,    # relaxation parameter (>=1) controls influence of prior steps\n",
        "    lambda_sparse=1e-3,  # sparsity regularization weight (encourages sparse masks)\n",
        "    optimizer_params=dict(lr=2e-2),\n",
        "    mask_type='sparsemax'\n",
        ")\n",
        "\n",
        "print(\"TabNet params:\", tabnet_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD7Qn8M_qtMW",
        "outputId": "5e61729f-6214-4590-c124-efd1a7f8cbed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TabNet params: {'n_d': 8, 'n_a': 8, 'n_steps': 5, 'gamma': 1.5, 'lambda_sparse': 0.001, 'optimizer_params': {'lr': 0.02}, 'mask_type': 'sparsemax'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: instantiate and train TabNet\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "clf = TabNetClassifier(\n",
        "    n_d=tabnet_params['n_d'],\n",
        "    n_a=tabnet_params['n_a'],\n",
        "    n_steps=tabnet_params['n_steps'],\n",
        "    gamma=tabnet_params['gamma'],\n",
        "    lambda_sparse=tabnet_params['lambda_sparse'],\n",
        "    optimizer_params=tabnet_params['optimizer_params'],\n",
        "    mask_type=tabnet_params['mask_type'],\n",
        "    device_name=device,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# fit: note TabNet accepts numpy arrays; we pass both train and validation so it can early stop\n",
        "clf.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
        "    eval_name=['train','valid'],\n",
        "    eval_metric=['accuracy'],\n",
        "    max_epochs=200,\n",
        "    patience=10,\n",
        "    batch_size=256,\n",
        "    virtual_batch_size=64\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDGGLwqRqvW2",
        "outputId": "a15bb88c-7716-4cdc-c29a-baf0a86a21a2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.7102  | train_accuracy: 0.854   | valid_accuracy: 0.86533 |  0:00:01s\n",
            "epoch 1  | loss: 0.31819 | train_accuracy: 0.88911 | valid_accuracy: 0.90067 |  0:00:02s\n",
            "epoch 2  | loss: 0.22843 | train_accuracy: 0.91867 | valid_accuracy: 0.924   |  0:00:03s\n",
            "epoch 3  | loss: 0.19889 | train_accuracy: 0.94111 | valid_accuracy: 0.93867 |  0:00:05s\n",
            "epoch 4  | loss: 0.16232 | train_accuracy: 0.95711 | valid_accuracy: 0.94333 |  0:00:06s\n",
            "epoch 5  | loss: 0.16408 | train_accuracy: 0.96022 | valid_accuracy: 0.944   |  0:00:07s\n",
            "epoch 6  | loss: 0.15471 | train_accuracy: 0.95667 | valid_accuracy: 0.94333 |  0:00:08s\n",
            "epoch 7  | loss: 0.13962 | train_accuracy: 0.96178 | valid_accuracy: 0.95133 |  0:00:10s\n",
            "epoch 8  | loss: 0.15498 | train_accuracy: 0.93422 | valid_accuracy: 0.92733 |  0:00:11s\n",
            "epoch 9  | loss: 0.14305 | train_accuracy: 0.95333 | valid_accuracy: 0.94267 |  0:00:12s\n",
            "epoch 10 | loss: 0.13987 | train_accuracy: 0.96711 | valid_accuracy: 0.95533 |  0:00:13s\n",
            "epoch 11 | loss: 0.15368 | train_accuracy: 0.96178 | valid_accuracy: 0.94867 |  0:00:15s\n",
            "epoch 12 | loss: 0.14568 | train_accuracy: 0.96444 | valid_accuracy: 0.95933 |  0:00:16s\n",
            "epoch 13 | loss: 0.12957 | train_accuracy: 0.96089 | valid_accuracy: 0.94733 |  0:00:17s\n",
            "epoch 14 | loss: 0.12045 | train_accuracy: 0.97    | valid_accuracy: 0.96933 |  0:00:18s\n",
            "epoch 15 | loss: 0.11433 | train_accuracy: 0.97467 | valid_accuracy: 0.97067 |  0:00:20s\n",
            "epoch 16 | loss: 0.12358 | train_accuracy: 0.97022 | valid_accuracy: 0.96133 |  0:00:21s\n",
            "epoch 17 | loss: 0.1229  | train_accuracy: 0.96978 | valid_accuracy: 0.96733 |  0:00:22s\n",
            "epoch 18 | loss: 0.1134  | train_accuracy: 0.97867 | valid_accuracy: 0.97333 |  0:00:23s\n",
            "epoch 19 | loss: 0.10069 | train_accuracy: 0.97822 | valid_accuracy: 0.97267 |  0:00:24s\n",
            "epoch 20 | loss: 0.11932 | train_accuracy: 0.97844 | valid_accuracy: 0.974   |  0:00:25s\n",
            "epoch 21 | loss: 0.1191  | train_accuracy: 0.96311 | valid_accuracy: 0.956   |  0:00:26s\n",
            "epoch 22 | loss: 0.11593 | train_accuracy: 0.96044 | valid_accuracy: 0.958   |  0:00:28s\n",
            "epoch 23 | loss: 0.11289 | train_accuracy: 0.97733 | valid_accuracy: 0.97067 |  0:00:31s\n",
            "epoch 24 | loss: 0.11836 | train_accuracy: 0.97533 | valid_accuracy: 0.96733 |  0:00:33s\n",
            "epoch 25 | loss: 0.11446 | train_accuracy: 0.96978 | valid_accuracy: 0.96933 |  0:00:35s\n",
            "epoch 26 | loss: 0.11392 | train_accuracy: 0.98022 | valid_accuracy: 0.966   |  0:00:36s\n",
            "epoch 27 | loss: 0.11486 | train_accuracy: 0.97289 | valid_accuracy: 0.972   |  0:00:37s\n",
            "epoch 28 | loss: 0.11361 | train_accuracy: 0.97667 | valid_accuracy: 0.976   |  0:00:38s\n",
            "epoch 29 | loss: 0.12679 | train_accuracy: 0.976   | valid_accuracy: 0.97333 |  0:00:40s\n",
            "epoch 30 | loss: 0.11953 | train_accuracy: 0.97622 | valid_accuracy: 0.96867 |  0:00:41s\n",
            "epoch 31 | loss: 0.10624 | train_accuracy: 0.98489 | valid_accuracy: 0.98067 |  0:00:42s\n",
            "epoch 32 | loss: 0.10925 | train_accuracy: 0.98311 | valid_accuracy: 0.98067 |  0:00:43s\n",
            "epoch 33 | loss: 0.10873 | train_accuracy: 0.98111 | valid_accuracy: 0.98    |  0:00:44s\n",
            "epoch 34 | loss: 0.11921 | train_accuracy: 0.982   | valid_accuracy: 0.97533 |  0:00:45s\n",
            "epoch 35 | loss: 0.11154 | train_accuracy: 0.97089 | valid_accuracy: 0.96667 |  0:00:47s\n",
            "epoch 36 | loss: 0.12869 | train_accuracy: 0.954   | valid_accuracy: 0.95533 |  0:00:48s\n",
            "epoch 37 | loss: 0.12308 | train_accuracy: 0.97311 | valid_accuracy: 0.97267 |  0:00:49s\n",
            "epoch 38 | loss: 0.10909 | train_accuracy: 0.98089 | valid_accuracy: 0.978   |  0:00:50s\n",
            "epoch 39 | loss: 0.1111  | train_accuracy: 0.98844 | valid_accuracy: 0.97933 |  0:00:51s\n",
            "epoch 40 | loss: 0.10149 | train_accuracy: 0.98756 | valid_accuracy: 0.98    |  0:00:53s\n",
            "epoch 41 | loss: 0.10171 | train_accuracy: 0.97822 | valid_accuracy: 0.968   |  0:00:54s\n",
            "\n",
            "Early stopping occurred at epoch 41 with best_epoch = 31 and best_valid_accuracy = 0.98067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Evaluate\n",
        "val_preds = clf.predict(X_val).reshape(-1)\n",
        "print(\"Accuracy:\", accuracy_score(y_val, val_preds))\n",
        "print(\"F1      :\", f1_score(y_val, val_preds))\n",
        "print(\"Precision:\", precision_score(y_val, val_preds))\n",
        "print(\"Recall   :\", recall_score(y_val, val_preds))\n",
        "\n",
        "# TabNet global feature importance (sum of masks across steps and samples)\n",
        "try:\n",
        "    fi = clf.feature_importances_\n",
        "    feat_imp = pd.Series(fi, index=feature_cols).sort_values(ascending=False)\n",
        "    print(\"\\nFeature importances (global):\\n\", feat_imp)\n",
        "    # quick bar plot\n",
        "    plt.figure(figsize=(6,3))\n",
        "    feat_imp.plot.bar()\n",
        "    plt.title(\"TabNet global feature importances\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"feature_importances_ not available:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "Y3D59GHQq1KT",
        "outputId": "530be47b-54b6-4e7d-fa06-db95bc9f352e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9806666666666667\n",
            "F1      : 0.9808074123097287\n",
            "Precision: 0.9814569536423841\n",
            "Recall   : 0.9801587301587301\n",
            "\n",
            "Feature importances (global):\n",
            " f0    0.453079\n",
            "f1    0.294981\n",
            "f3    0.174057\n",
            "f2    0.028819\n",
            "f5    0.027620\n",
            "f4    0.021443\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: explain() — per-sample masks\n",
        "# explain() typically returns (preds, masks) depending on version; masks shape: (n_samples, n_steps, n_features)\n",
        "explain_output = None\n",
        "try:\n",
        "    explain_output = clf.explain(X_val)\n",
        "    # interpretation of returned structure depends on TabNet version:\n",
        "    # often explain_output = (feature_importances, masks) or (masks, something)\n",
        "    # we'll try to detect masks:\n",
        "    if isinstance(explain_output, tuple) or isinstance(explain_output, list):\n",
        "        # search for a 3D array in returned tuple/list\n",
        "        masks = None\n",
        "        for elt in explain_output:\n",
        "            if hasattr(elt, 'shape') and len(getattr(elt,'shape')) == 3:\n",
        "                masks = elt\n",
        "                break\n",
        "    elif isinstance(explain_output, dict) and 'masks' in explain_output:\n",
        "        masks = explain_output['masks']\n",
        "    else:\n",
        "        masks = None\n",
        "    print(\"Masks type:\", type(masks), \"shape (if available):\", getattr(masks, 'shape', None))\n",
        "except Exception as e:\n",
        "    print(\"explain() failed or returned unexpected format:\", e)\n",
        "    masks = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99_XLGd9rT4k",
        "outputId": "ba4c5eea-96fa-40eb-908e-da0832f161e9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masks type: <class 'NoneType'> shape (if available): None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: visualize masks for first few samples\n",
        "if masks is None:\n",
        "    print(\"No masks available from explain() on this TabNet version. Check clf.feature_importances_.\")\n",
        "else:\n",
        "    n_samples_to_show = 4\n",
        "    n_steps = masks.shape[1]\n",
        "    n_features = masks.shape[2]\n",
        "    feature_names = feature_cols\n",
        "    for i in range(n_samples_to_show):\n",
        "        sample_mask = masks[i]  # shape (n_steps, n_features)\n",
        "        plt.figure(figsize=(8, 1.6 * n_steps))\n",
        "        plt.imshow(sample_mask, aspect='auto')\n",
        "        plt.colorbar(label='mask value')\n",
        "        plt.yticks(ticks=np.arange(n_steps), labels=[f\"step_{s+1}\" for s in range(n_steps)])\n",
        "        plt.xticks(ticks=np.arange(n_features), labels=feature_names, rotation=45)\n",
        "        plt.title(f\"TabNet masks — validation sample {i} (label={y_val[i]})\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-F90ApjrcJq",
        "outputId": "ade10008-3808-40cf-918d-819ce88e72ad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No masks available from explain() on this TabNet version. Check clf.feature_importances_.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: aggregate masks across validation set (mean across samples)\n",
        "if masks is not None:\n",
        "    avg_mask = masks.mean(axis=0)  # shape (n_steps, n_features)\n",
        "    plt.figure(figsize=(8, 1.6 * avg_mask.shape[0]))\n",
        "    plt.imshow(avg_mask, aspect='auto')\n",
        "    plt.colorbar(label='avg mask value')\n",
        "    plt.yticks(ticks=np.arange(avg_mask.shape[0]), labels=[f\"step_{s+1}\" for s in range(avg_mask.shape[0])])\n",
        "    plt.xticks(ticks=np.arange(avg_mask.shape[1]), labels=feature_cols, rotation=45)\n",
        "    plt.title(\"Average TabNet mask across validation samples (per step)\")\n",
        "    plt.show()\n",
        "    # Also show sum across steps to see overall attention per feature\n",
        "    overall_per_feature = avg_mask.sum(axis=0)\n",
        "    plt.figure(figsize=(6,3))\n",
        "    pd.Series(overall_per_feature, index=feature_cols).sort_values(ascending=False).plot.bar()\n",
        "    plt.title(\"Sum of average masks across steps (feature total attention)\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3xTyP4KJrfF0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: inspect masks vs feature values for a specific sample\n",
        "if masks is not None:\n",
        "    idx = 0  # pick sample index from validation set\n",
        "    print(\"Feature values for sample\", idx)\n",
        "    for fn, val in zip(feature_cols, X_val[idx]):\n",
        "        print(f\"  {fn}: {val:.3f}\")\n",
        "    print(\"\\nPer-step mask values:\")\n",
        "    df_mask = pd.DataFrame(masks[idx], columns=feature_cols, index=[f\"step_{i+1}\" for i in range(masks.shape[1])])\n",
        "    display(df_mask)\n",
        "else:\n",
        "    print(\"Masks not available to inspect.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vivYfUrrqNp",
        "outputId": "f5e55ea0-2793-41d2-9a98-4df3e7b4b453"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masks not available to inspect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: ablation: compare sparse vs non-sparse masks\n",
        "# Re-train a small model with lambda_sparse=0 (no sparsity regularization) for contrast\n",
        "clf_dense = TabNetClassifier(\n",
        "    n_d=8, n_a=8, n_steps=5, gamma=tabnet_params['gamma'],\n",
        "    lambda_sparse=0.0, optimizer_params=tabnet_params['optimizer_params'],\n",
        "    mask_type='sparsemax', device_name=device, verbose=0\n",
        ")\n",
        "clf_dense.fit(X_train, y_train, eval_set=[(X_val,y_val)], eval_name=['valid'], eval_metric=['accuracy'],\n",
        "              max_epochs=50, patience=5, batch_size=256, virtual_batch_size=64)\n",
        "\n",
        "# try explain\n",
        "try:\n",
        "    explain_dense = clf_dense.explain(X_val)\n",
        "    masks_dense = None\n",
        "    if isinstance(explain_dense, tuple) or isinstance(explain_dense, list):\n",
        "        for elt in explain_dense:\n",
        "            if hasattr(elt, 'shape') and len(getattr(elt,'shape'))==3:\n",
        "                masks_dense = elt\n",
        "                break\n",
        "except Exception as e:\n",
        "    masks_dense = None\n",
        "    print(\"explain failed for dense model:\", e)\n",
        "\n",
        "# Compare avg sparsity (proportion of near-zero entries) if both masks exist\n",
        "if masks is not None and masks_dense is not None:\n",
        "    avg_mask_sparse = masks.mean()\n",
        "    avg_mask_dense = masks_dense.mean()\n",
        "    # compute proportion of small entries\n",
        "    prop_zero_sparse = (np.abs(masks) < 1e-3).mean()\n",
        "    prop_zero_dense = (np.abs(masks_dense) < 1e-3).mean()\n",
        "    print(\"Proportion tiny-mask entries (sparse model):\", prop_zero_sparse)\n",
        "    print(\"Proportion tiny-mask entries (dense model):\", prop_zero_dense)\n",
        "    # Visualize avg masks side by side for comparison (first step)\n",
        "    plt.figure(figsize=(10,3))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(masks.mean(axis=0), aspect='auto'); plt.title('avg masks (sparse lambda>0)')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(masks_dense.mean(axis=0), aspect='auto'); plt.title('avg masks (lambda=0)')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Masks missing for one or both models; cannot compare directly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILseqn8CrsrD",
        "outputId": "0ed10e08-8c5c-4e03-9c40-fe34cb17c25d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 27 with best_epoch = 22 and best_valid_accuracy = 0.97867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masks missing for one or both models; cannot compare directly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary — Practical principles from the demo\n",
        "\n",
        "- TabNet uses **sequential attention** (masks) to pick feature subsets per decision step; this gives both model power and interpretability.\n",
        "- Key hyperparameters:\n",
        "  - `n_steps`: number of sequential views; more steps allow more complex reasoning but increase complexity.\n",
        "  - `n_d,n_a`: width of latent embeddings; control capacity.\n",
        "  - `lambda_sparse`: pushes the model to use fewer features per step (improves interpretability).\n",
        "  - `mask_type`: usually `'sparsemax'` (or `'entmax'`) to produce sparse attention.\n",
        "- `explain()` (masks) gives per-sample per-step attention — use it to inspect why model predicted a certain way.\n",
        "- Aggregate masks to find common patterns (which features are important globally, and which steps focus on which features).\n",
        "- In a practical pipeline, TabNet’s masks can be combined with an adaptive sampling or reweighting scheme (like ASS) to improve subgroup fairness — masks tell you *what* it looks at, ASS tells TabNet *where* to focus data collection."
      ],
      "metadata": {
        "id": "5kJ_tkumr-9a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZP4ZxpsfsAhK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}